
dropout_rate: 0.5
cell_size: 64
attn_size: 32
embedding_size: 32

layers: 1
bidir: True
use_lstm: True

weight_scale: 0.1

data_dir: experiments/WMT17/data_2017
model_dir: experiments/WMT17/subwords_tiny

batch_size: 32

optimizer: 'adam'
learning_rate: 0.001
#optimizer: 'sgd'
#learning_rate: 1.0
#learning_rate_decay_factor: 0.95
#decay_every_n_epoch: 1
#decay_after_n_epoch: 3

steps_per_checkpoint: 400
steps_per_eval: 400
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 100
batch_mode: 'standard'
read_ahead: 10

attention_type: 'global'

encoders:
  - name: mt
    max_len: 39  # 99% coverage
    embeddings_on_cpu: False
    layers: 2
  - name: src
    max_len: 37
    layers: 2

decoders:
  - name: mt
    ext: pe
    max_len: 40
    aggregation_method: concat
    tie_embeddings: True
    embeddings_on_cpu: False
    cell_size: 64

train_prefix: train.subwords
vocab_prefix: vocab.subwords
dev_prefix: dev.subwords
