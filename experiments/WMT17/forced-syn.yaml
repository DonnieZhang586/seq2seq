
dropout_rate: 0.5
cell_size: 128
attn_size: 128
embedding_size: 128

layers: 1
bidir: True
use_lstm: True

weight_scale: 0.1

data_dir: experiments/WMT17/data
model_dir: experiments/WMT17/forced_syn

batch_size: 32

optimizer: 'sgd'
learning_rate: 1.0
learning_rate_decay_factor: 0.8
decay_every_n_epoch: 0.5
decay_after_n_epoch: 1

steps_per_checkpoint: 200
steps_per_eval: 200
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 8
batch_mode: 'standard'
read_ahead: 10

encoders:
  - name: mt
    align_edits: True
    attention_type: 'local'
    attention_window_size: 0
    max_len: 36

decoders:
  - name: edits
    max_len: 45

pred_edits: True
ref_ext: pe

train_prefix: train.concat
vocab_prefix: vocab.concat
