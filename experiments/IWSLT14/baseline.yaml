# Same parameters as: https://github.com/facebookresearch/MIXER
# FIXME:
# - different batch iteration method
# - additional projection before LSTM (instead of using custom LSTM)
# - possibly broken attention mechanism (especially energy function)
# TODO:
# - compare results with torch code

dropout_rate: 0.0
cell_size: 256
embedding_size: 256
layers: 1
bidir: False
use_lstm: True
steps_per_checkpoint: 1000
steps_per_eval: 4000
max_steps: 0
# optimizer: 'adam'
optimizer: 'sgd'
learning_rate: 0.2
weight_scale: 0.05
max_gradient_norm: 10
batch_size: 32
max_output_len: 25

data_dir: experiments/IWSLT14/data
model_dir: experiments/IWSLT14/baseline
max_dev_size: 2000

encoders:
  - name: de

decoder:
    name: en
