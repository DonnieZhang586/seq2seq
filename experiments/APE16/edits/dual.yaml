
dropout_rate: 0.5
cell_size: 128
attn_size: 128
embedding_size: 64

layers: 1
bidir: True
use_lstm: True

weight_scale: 0.1

data_dir: experiments/APE16/edits/data_dual_nosub
model_dir: experiments/APE16/edits/dual

batch_size: 32

optimizer: 'sgd'
#learning_rate: 0.0003
learning_rate: 1.0
learning_rate_decay_factor: 0.95
decay_every_n_epoch: 1
decay_after_n_epoch: 5

steps_per_checkpoint: 200
steps_per_eval: 200
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 80
batch_mode: 'standard'
read_ahead: 10

encoders:
  - name: de
    ext: mt
    align_edits: True
    attention_type: 'local'
    attention_window_size: 0
    max_len: 36

max_len: 39
decoders:   # first decoder is the main one
  - name: ops
    ext: edits.ops

    embedding_size: 32

    skip_update: True
    loss_ratio: 2.0
    multi_loss_strategy: random
    ignore_dels: True
    force_keep: True
    #input_aggregation: only_words   # only_words, only_ops, concat, sum
    output_aggregation: only_words

  - name: de
    ext: edits.words

ref_ext: pe    # BLEU eval with respect to this reference
pred_edits: True
dual_output: True
tie_embeddings: True
