
dropout_rate: 0.5
cell_size: 128
attn_size: 128
embedding_size: 128

layers: 1
bidir: True
use_lstm: True

weight_scale: 0.1

data_dir: experiments/APE16/edits/data
model_dir: experiments/APE16/edits/multitask

batch_size: 32

optimizer: 'sgd'
#learning_rate: 0.0003
learning_rate: 1.0
learning_rate_decay_factor: 0.95
decay_every_n_epoch: 1
decay_after_n_epoch: 3
#decay_every_n_epoch: 0.1
#decay_after_n_epoch: 0.3

steps_per_checkpoint: 200
steps_per_eval: 200
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 80
batch_mode: 'standard'
read_ahead: 10

tasks:
  - name: post_editing
    ref_ext: pe
    pred_edits: True
    encoders:
      - name: de
        ext: mt
        align_edits: True
        attention_type: 'local'
        attention_window_size: 0
        max_len: 36

    decoders:
      - name: edits
        #name: de
        ext: edits
        max_len: 45
        #tie_embeddings: False
        #input_attention: False
        #skip_updates: True
        #lstm_input: words         # concat, sum, words, ops
        #prediction_input: ops     # concat, sum, words, ops, none
        #split_ops: True

  - name: translation
    encoders:
      - name: de
        ext: mt
        max_len: 36

    decoders:
      - name: src
        max_len: 33

  - name: post_editing_bis
    encoders:
      - name: de
        ext: mt
        max_len: 36

    decoders:
      - name: de    # TODO: share embeddings but not decoder
        ext: pe
        max_len: 37
        tie_embeddings: True

maxout: True
embeddings_on_cpu: False
#train_prefix: train.random
