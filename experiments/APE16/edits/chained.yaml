
dropout_rate: 0.5
cell_size: 128
attn_size: 128
embedding_size: 128

layers: 1
bidir: True
use_lstm: True

max_output_len: 45   # 99% coverage
max_input_len: 36
weight_scale: 0.1

data_dir: experiments/APE16/edits/data
model_dir: experiments/APE16/edits/chained
# load: [experiments/APE16/edits/baseline_chained_pretrain/checkpoints/best,
#        experiments/APE16/edits/baseline_chained_pretrain_PE/checkpoints/best]

batch_size: 32

#optimizer: 'adam'
#reset: True
learning_rate: 1.0
learning_rate_decay_factor: 0.96
decay_every_n_epoch: 1
decay_after_n_epoch: 5

steps_per_checkpoint: 200
steps_per_eval: 200
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 80
batch_mode: 'standard'
read_ahead: 10

encoders:
  - name: mt
    align_edits: True
    attention_type: 'local'
    attention_window_size: 0

  - name: src
    attention_type: 'global'
    cell_size: 256
    attn_size: 256

decoder:
    name: edits
    pred_edits: True

train_prefix: train
vocab_prefix: vocab

chained_encoders: True
chaining_strategy: map_attns   # share_states, share_outputs, concat_attns, concat_states, sum_states, map_attns, map_states
more_dropout: True
chaining_non_linearity: True
chaining_bias: True
chaining_loss_ratio: 1.0
