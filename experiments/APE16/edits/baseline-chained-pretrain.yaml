
dropout_rate: 0.5
cell_size: 256
attn_size: 256
embedding_size: 256

layers: 1
bidir: True
use_lstm: True

max_output_len: 36   # 99% coverage
max_input_len: 33
weight_scale: 0.1

data_dir: experiments/APE16/edits/data
model_dir: experiments/APE16/edits/baseline_chained_pretrain

batch_size: 32

optimizer: 'adam'
# learning_rate: 1.0
# learning_rate_decay_factor: 0.95
# decay_every_n_epoch: 1
# decay_after_n_epoch: 3

steps_per_checkpoint: 200
steps_per_eval: 200
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 80
batch_mode: 'standard'
read_ahead: 10

encoders:
  - name: src
    attention_type: 'global'

decoder:
    name: mt

train_prefix: train.concat
vocab_prefix: vocab.train