# 40000 subword units
# beam size of 12
# pre-training with 4m.*
# finetuning with 500k.* + 20x train.*

dropout_rate: 0.5
cell_size: 512
attn_size: 512
embedding_size: 256

layers: 1
bidir: True
use_lstm: False
max_output_len: 42
max_input_len: 42
weight_scale: 0.1

data_dir: experiments/APE16/data_plus
model_dir: experiments/APE16/xent_translation_plus
log_file: experiments/APE16/xent_translation_plus/log.txt
batch_size: 32

loss_function: 'xent'
optimizer: 'adam'

sgd_after_n_epoch: 4
learning_rate: 0.2
learning_rate_decay_factor: 0.9
decay_every_n_epoch: 1
decay_after_n_epoch: 5

steps_per_checkpoint: 500
steps_per_eval: 500
score_function: corpus_scores_ter

max_gradient_norm: 1.0
max_epochs: 20
batch_mode: 'standard'
read_ahead: 10

encoders:
  - name: mt
  - name: src

decoder:
    name: pe
