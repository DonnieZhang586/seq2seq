label: 'debug'
description: "small BTEC config"

dropout_rate: 0.0
cell_size: 128
attn_size: 128
embedding_size: 64

layers: 1
bidir: True
use_lstm: False
weight_scale: 0.1  # FIXME

data_dir: data/btec_char
model_dir: models/debug_char
batch_size: 32
dev_prefix: dev

optimizer: 'adam'
learning_rate: 0.001  # FIXME
#optimizer: 'sgd'
#learning_rate: 0.001
#sgd_learning_rate: 0.001

#sgd_after_n_epoch: 20
#sgd_learning_rate: 0.2
learning_rate_decay_factor: 0.9
decay_every_n_epoch: 1
decay_after_n_epoch: 5

steps_per_checkpoint: 200
steps_per_eval: 200
score_function: corpus_scores

max_gradient_norm: 1.0
batch_mode: 'standard'
read_ahead: 10
max_len: 25

encoders:
  - name: fr
    #ext: char.fr
    #max_len: 100
    #character_level: True
    #maxout_stride: 3
    convolutions: [32, 32, 32]

decoders:
  - name: en
    #ext: char.en
    #character_level: True
    #max_len: 100
    maxout: True

#keep_best: 100
#input_attention: False
#use_previous_word: False
#maxout: False
#tie_embeddings: True
