label: 'BTEC baseline'
description: "multi-speaker baseline on BTEC, same configuration as Berard et al. 2016"

data_dir: data/BTEC_speech
model_dir: models/BTEC_speech/baseline_char
train_prefix: train.concat.shuf
max_train_size: 40000
vocab_prefix: vocab.w2c

optimizer: adam
learning_rate: 0.001
batch_size: 64
batch_mode: standard
read_ahead: 10
max_gradient_norm: 5.0

steps_per_checkpoint: 1000
steps_per_eval: 1000
max_steps: 60000

cell_size: 256
attn_size: 256
cell_type: LSTM

encoders:
  - name: feats41
    embedding_size: 41
    layers: 3
    conv_filters: [32, 32]
    conv_size: [3, 3]
    conv_strides: [2, 2]
    conv_activation: relu
    # inter_layers: [512, 512, 512]
    # inter_layer_activation: relu
    # rnn_output_dropout: 0.4
    binary: True
    max_len: 600
    input_layers: [256, 256]
    bidir_projection: True
    final_state: concat_last
    train_initial_states: False
    bidir: True

decoders:
  - name: en
    layers: 2
    embedding_size: 64
    max_len: 120
    pred_maxout_layer: False
    rnn_feed_attn: True
    use_previous_word: False
    pred_embed_proj: False
    update_first: False
    character_level: True

use_dropout: True
pervasive_dropout: True
attn_dropout: 0.4
rnn_input_dropout: 0.4
initial_state_dropout: 0.4
input_layer_dropout: 0.4
use_lstm_full_state: False

