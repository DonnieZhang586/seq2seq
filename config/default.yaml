# BIOHAZARD -- DO NOT EDIT THIS FILE

# SGD parameters
learning_rate: 0.5             # initial learning rate
learning_rate_decay_factor: 0.99
decay_every_n_epoch: null
decay_after_n_epoch: 0         # start decaying learning rate after this many epochs
decay_if_no_progress: null     # measure average loss over this many checkpoints, and decay if no progress
sgd_after_n_epoch: null        # use SGD optimizer after this many epochs (instead of adam/adadelta)
reinforce_after_n_epoch: null  # switch to a reinforce loss after this many epochs TODO

# batch iteration parameters
batch_mode: 'standard'   # standard, random, or strict
shuffle_data: True       # shuffle dataset at each new epoch
read_ahead: 10           # number of batches to read ahead and sort

# training parameters
max_gradient_norm: 5.0   # clip gradients to this norm
dropout_rate: 0.0        # dropout rate applied to the RNN units
max_train_size: 0        # maximum size of the training data (0 for unlimited)
max_dev_size: 0          # maximum size of the dev data
steps_per_checkpoint: 1000   # number of updates between each checkpoint
steps_per_eval: 1000     # number of updates between each BLEU eval (on dev set)
eval_burn_in: 0          # minimum number of steps before starting BLEU eval
max_steps: 0             # maximum number of updates before stopping
max_epochs: 0            # maximum number of epochs before stopping
keep_best: 4             # number of best checkpoints to keep
feed_previous: 0.0       # randomly feed previous output instead of groundtruth to decoder during training
optimizer: 'sgd'         # 'sgd', 'adadelta', or 'adam'
# TODO: add min_learning_rate parameter

# reinforce parameters
loss_function: 'xent'    # 'xent' or 'reinforce'
baseline_steps: 0        # pre-training steps for reward baseline
reinforce_baseline: True # train a reward baseline for reinforce
rollouts: null
partial_rewards: False
reward_function: 'sentence_bleu'

# model (each one of these settings can be defined specifically in `encoders` and `decoder`, or generally here)
batch_size: 80           # training batch size
cell_size: 1000          # size of the RNN cells
embedding_size: 620      # size of the embeddings
attn_size: 1000          # size of the attention layer
layers: 1                # number of RNN layers per encoder and decoder
bidir: True              # use bidirectional encoders
attention_window_size: 0 # if positive, use a local attention mechanism with this window size
attention_filters: 0     # number of convolution filters to use in the attention mechanism
attention_filter_length: 0  # length of the convolution filters
vocab_size: 0            # number of symbols of each encoder and decoder (0: size of vocab files)
use_lstm: False          # use LSTM units instead of GRU units
binary: False            # input file is binary (contains vector features)
character_level: False   # input sequence is at the character level
load_embeddings: []      # load pre-trained embeddings for those extensions
time_pooling: null       # ratios of time steps to skip between layers
pooling_avg: False       # average previous steps instead of skipping them
input_layers: []         # fully connected layers between the embeddings and the RNN
residual_connections: False  # connections between nth and nth+2 layer for easier gradient flow
weight_scale: null       # if not null, initialize all weights to a normal distribution with this stdev

# data
data_dir: data           # directory containing the training data
model_dir: model         # directory where the model will be saved (checkpoints and eval outputs)
train_prefix: train      # name of the training corpus
script_dir: scripts      # directory where the scripts are kepts (in particular the scoring scripts)
dev_prefix: [dev]        # names of the development corpora
vocab_prefix: vocab      # name of the vocabulary files
embedding_prefix: vectors  # name of the embeddings files
checkpoints: []          # list of checkpoints to load (in this specific order) after main checkpoint
max_input_len: 50        # maximum length of the input sequences (strongly affects memory usage)

# decoding
score_function: corpus_scores # name of the main scoring function (used for selecting models)
remove_unk: False        # remove UNK symbols from the decoder output
lm_file: null            # path to a language model file (in arpa format) to use during decoding
lm_weight: 0.2           # weight of the language model in the log-linear model
beam_size: 1             # beam size for decoding (decoder is greedy by default)
ensemble: False          # use an ensemble of models while decoding (specified by the --checkpoints parameter)
output: null             # output file for decoding (writes to standard output by default)
max_output_len: 50       # maximum length of the sequences generated by the decoder (strongly affects decoding speed)
len_normalization: 1.0   # length normalization coefficient used in beam-search decoder
softmax_temperature: 1.0 # temperature to use when decoding with beam-search (temperature of 1.0 is regular softmax)
early_stopping: True     # reduce beam-size each time a finished hypothesis is encountered (affects decoding speed)
use_edits: False         # output is a sequence of edits, apply those edits before decoding/evaluating

# general
gpu_id: 0                # index of the GPU to use
no_gpu: False            # don't use any GPU
allow_growth: True       # allow GPU memory allocation to change during runtime
mem_fraction: 1.0        # maximum fraction of GPU memory to use
freeze_variables: []     # list of variables to freeze during training
log_file: null           # log to this file instead of standard output
parallel_iterations: 16  # parameter of Tensorflow's while_loop
swap_memory: True        # parameter of Tensorflow's while_loop
max_to_keep: 3           # keep that many latest checkpoints
keep_every_n_hours: 0    # keep checkpoints every n hours

encoders:                # this is a list (you can specify several encoders)
  - name: fr             # each encoder or decoder has a name (used for naming variables) and an extension (for files)
    ext: fr              # If undefined, `ext` is the same as `name`

decoder:                 # Each encoder or decoder can redefine its own values for a number of parameters,
    name: en             # including `cell_size`, `embedding_size` and `attn_size`
