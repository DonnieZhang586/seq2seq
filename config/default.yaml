# BIOHAZARD -- DO NOT EDIT THIS FILE

label: "default"
description: |
  default configuration
  next line of description
  last line

# SGD parameters
learning_rate: 0.001           # initial learning rate
sgd_learning_rate: 1.0         # SGD can start at a different learning rate (useful for switching between Adam and SGD)
learning_rate_decay_factor: 1.0  # decay the learning rate by this factor at a given frequency
decay_every_n_epoch: null      # self explanatory, can be lower than one (e.g. 0.5 for every half epoch)
decay_after_n_epoch: 0         # start decaying learning rate after this many epochs
decay_if_no_progress: null     # measure average loss over this many checkpoints, and decay if no progress
sgd_after_n_epoch: null        # start using SGD optimizer after this many epochs (instead of Adam/AdaDelta)
min_learning_rate: 0.000001    # stop training when learning rate is smaller than this

# training parameters
max_gradient_norm: 5.0   # clip gradients to this norm (prevents exploding gradient)
dropout_rate: 0.0        # dropout rate applied to the RNN inputs (after embedding), and to the decoder's initial state
steps_per_checkpoint: 1000   # number of SGD updates between each checkpoint
steps_per_eval: 1000     # number of SGD updates between each BLEU eval (on dev set)
eval_burn_in: 0          # minimum number of updates before starting BLEU eval
max_steps: 0             # maximum number of updates before stopping
max_epochs: 0            # maximum number of epochs before stopping
keep_best: 4             # number of best checkpoints to keep (based on BLEU score on dev set)
feed_previous: 0.0       # randomly feed prev output instead of ground truth to decoder during training ([0,1] proba)
optimizer: adam          # which training algorithm to use ('sgd', 'adadelta', or 'adam')

# initialization parameters
weight_scale: null       # if null, initialize weights to TF defaults, otherwise to normal distribution with this stdev
initializer: null        # if 'uniform' initialize uniformly between [-weight_scale, +weight_scale] instead

# batch iteration parameters
batch_size: 80           # batch size (during training and greedy decoding)
batch_mode: standard     # standard (cycle through train set) or random (sample from train set)
shuffle: True            # shuffle dataset at each new epoch
read_ahead: 10           # number of batches to read ahead and sort by sequence length (can speed up training)

# reinforce parameters
loss_function: xent      # 'xent' or 'reinforce'
reward_function: sentence_bleu  # sentence-level reward function used by reinforce (see 'evaluation' module)
reinforce_after_n_epoch: null  # TODO: start using reinforce instead of xent after this many epochs
use_baseline: True       # train a reward baseline for reinforce
baseline_steps: 0        # pre-training steps for the reward baseline
baseline_optimizer: adam       # if None, use the same one as the main model
baseline_learning_rate: 0.001  # likewise
baseline_layers: null      # TODO
baseline_activation: null  # default: linear

# model (each one of these settings can be defined specifically in 'encoders' and 'decoders', or generally here)
cell_size: 1000          # size of the RNN cells
embedding_size: 620      # size of the embeddings
attn_size: 1000          # size of the attention layer
layers: 1                # number of RNN layers per encoder and decoder
use_lstm: False          # use LSTM units instead of GRU units
character_level: False   # character-level sequences
max_len: 50              # max length of the input and output sequences (strongly affects speed and memory usage)

# encoder settings
bidir: True              # use bidirectional encoders
attention_type: global   # global, local, none, last_state, average
attention_window_size: 0 # window size for local attention mechanism
convolutions: null       # list of convolutions to perform on the input sequence
maxout_stride: null      # maxout layer with this stride on the input sequence (after convolutions)
trainable_initial_states: True  # whether the initial states of the encoder should be trainable parameters
concat_last_states: False
bidir_projection: False  # project bidirectional encoder states to cell_size (or just keep the concatenation)
time_pooling: null       # perform time pooling (skip states) between the layers of the encoder (list of layers - 1 ratios)
pooling_avg: True        # average or skip consecutive states
binary: False            # use binary input for the encoder (no vocab and no embeddings, see utils.read_binary_features)
attention_filters: 0
attention_filter_length: 0
input_layers: null       # list of fully connected layer sizes, applied before the encoder
attention_temperature: 1.0  # 1.0: true softmax (low values: uniform distribution, high values: argmax)

# decoder settings
tie_embeddings: False    # use transpose of the embedding matrix for output projection (requires 'output_extra_proj')
output_extra_proj: True  # project decoder output to embedding size before projecting to vocab size
state_zero: False        # first decoder state is the successor of the initial state (and not the initial state itself)
use_lstm_state: True     # use LSTM state instead of LSTM output for next word prediction
maxout: True             # parameters of the decoder
input_attention: True    # use attention as input to the LSTM
use_previous_word: True  # use previous word when predicting a new word
vanilla: False           # use a vanilla decoder (Bahdanau et al.), i.e. do state transition before next word prediction
attn_prev_word: False    # use the previous word in the attention model
softmax_temperature: 1.0 # TODO: temperature of the output softmax
pred_edits: False        # output is a sequence of edits, apply those edits before decoding/evaluating

# data
max_train_size: 0        # maximum size of the training data (0 for unlimited)
max_dev_size: 0          # maximum size of the dev data
max_test_size: 0         # maximum size of the test data
data_dir: data           # directory containing the training data
model_dir: model         # directory where the model will be saved (checkpoints and eval outputs)
train_prefix: train      # name of the training corpus
script_dir: scripts      # directory where the scripts are kepts (in particular the scoring scripts)
dev_prefix: [dev]        # names of the development corpora
vocab_prefix: vocab      # name of the vocabulary files
load: []                 # list of checkpoints to load (in this specific order) after main checkpoint

# decoding
score_function: corpus_scores # name of the main scoring function (used for selecting models)
remove_unk: False        # remove UNK symbols from the decoder output
beam_size: 1             # beam size for decoding (decoder is greedy by default)
ensemble: False          # use an ensemble of models while decoding (specified by the --checkpoints parameter)
output: null             # output file for decoding (writes to standard output by default)
len_normalization: 1.0   # length normalization coefficient used in beam-search decoder
early_stopping: True     # reduce beam-size each time a finished hypothesis is encountered (affects decoding speed)
raw_output: False

# general
gpu_id: 0                # index of the GPU to use (starts at zero)
no_gpu: False            # run on CPU only
allow_growth: True       # allow GPU memory allocation to change during runtime
mem_fraction: 1.0        # maximum fraction of GPU memory to use
freeze_variables: []     # list of variables to freeze during training
log_file: log.txt        # log to this file in addition to standard output
parallel_iterations: 16  # parameter of Tensorflow's while_loop
swap_memory: True        # parameter of Tensorflow's while_loop
max_to_keep: 1           # keep that many latest checkpoints
keep_every_n_hours: 0    # keep checkpoints every n hours
embeddings_on_cpu: True

encoders:                # this is a list (you can specify several encoders)
  - name: fr             # each encoder or decoder has a name (used for naming variables) and an extension (for files)
    ext: fr              # If undefined, `ext` is the same as `name`

decoders:                # Each encoder or decoder can redefine its own values for a number of parameters,
  - name: en             # including `cell_size`, `embedding_size` and `attn_size`
